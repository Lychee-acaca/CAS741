\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{ colorlinks, citecolor=black, filecolor=black, linkcolor=red,
    urlcolor=blue }
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X} \toprule {\bf Date} & {\bf Version}
& {\bf Notes}\\
\midrule
April 11, 2025 & 1.0 & Creation\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document ...

\section{Functional Requirements Evaluation}

\section{Nonfunctional Requirements Evaluation}

\subsection{Usability}

\section{Unit Testing}

A comprehensive unit testing process was conducted to validate the
functionality, robustness, and correctness of the system's core components. A
total of 89 unit test cases were executed, covering modules including R-wave
detection, I/O processing, general digital filtering, root mean square error
(RMSE) evaluation, mathematical operations, and data structures.

\begin{itemize}
    \item \textbf{Total Test Cases:} 89
    \item \textbf{Passed:} 89
    \item \textbf{Failed:} 0
    \item \textbf{Execution Time:} Approximately 2.18 seconds (with the longest
    individual test taking about 1.89 seconds)
\end{itemize}

All unit tests passed successfully, demonstrating high reliability and stability
of the implemented modules.

\section{Changes Due to Testing}

Testing revealed the need for more accessible runtime information, leading to
the addition of a logger module. To facilitate testing and debugging, the logger
was configured to output messages to a file instead of standard output.

\section{Automated Testing}

Automated testing is integrated into the development workflow via GitHub
Actions. The workflow is triggered on each push or pull request and performs the
following checks:

\begin{itemize}
    \item \textbf{cpplint:} Checks adherence to the Google C++ Style Guide.
    \item \textbf{cppcheck:} Performs static analysis to detect potential bugs
    and code quality issues.
    \item \textbf{Google Test:} Executes the full suite of unit tests described
    previously.
    \item \textbf{LCOV:} Generates code coverage reports to monitor test
    completeness.
\end{itemize}

This automated setup ensures consistent code quality, early bug detection, and
continuous test coverage monitoring throughout the development lifecycle.
		
\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}

Code coverage for this project is evaluated using the \texttt{lcov} tool.
Automated coverage reports are generated and uploaded through GitHub Actions to
Codecov. The coverage dashboard can be accessed at:

\begin{center}
\url{https://app.codecov.io/gh/Lychee-acaca/CAS741}
\end{center}

As of the current assessment, the overall test coverage is \textbf{95.01\%}. The
coverage statistics for each major source file are summarized below:

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{File} & \textbf{Lines Covered / Total} & \textbf{Coverage (\%)} \\
\hline
\texttt{RwaveDetect.cpp}         & 15 / 15    & 100.00\% \\
\texttt{annRMSE.cpp}             & 18 / 19    & 94.74\%  \\
\texttt{annRMSE.hpp}             & 1 / 1      & 100.00\% \\
\texttt{dataStructure.hpp}       & 91 / 94    & 96.81\%  \\
\texttt{generalDigitalFilter.cpp}& 24 / 24    & 100.00\% \\
\texttt{generalDigitalFilter.hpp}& 10 / 10    & 100.00\% \\
\texttt{io\_processing.cpp}       & 49 / 55    & 89.09\%  \\
\texttt{io\_processing.hpp}       & 3 / 3      & 100.00\% \\
\texttt{logger.cpp}              & 33 / 37    & 89.19\%  \\
\texttt{logger.hpp}              & 12 / 12    & 100.00\% \\
\texttt{mmath.cpp}               & 50 / 52    & 96.15\%  \\
\texttt{pantomp.cpp}             & 127 / 134  & 94.78\%  \\
\texttt{pantomp.hpp}             & 5 / 5      & 100.00\% \\
\hline
\end{tabular}
\end{center}

This high level of coverage demonstrates that most of the critical logic in the
project has been thoroughly tested.

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how did
    you resolve them?
  \item Which parts of this document stemmed from speaking to your client(s) or
  a proxy (e.g. your peers)? Which ones were not, and why?
  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)
\end{enumerate}

\end{document}