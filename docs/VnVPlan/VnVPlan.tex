\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{ colorlinks, citecolor=blue, filecolor=black, linkcolor=red,
    urlcolor=blue }
\usepackage[round]{natbib}
\usepackage{graphicx}

\input{../Comments}
\input{../Common}

\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\newcommand{\tcref}[1]{TC\ref{#1}} \newcounter{testcasenum}
\newcommand{\tthetestcasenum}{TM\thetestcasenum}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X} \toprule {\bf Date} & {\bf Version}
& {\bf Notes}\\
\midrule
Feb 21, 2025 & 1.0 & Creation\\
April 11, 2025 & 1.1 & Complete unit tests\\
\bottomrule
\end{tabularx}

% ~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation
% technique that has ever been devised.  The VnV plan should also be a
% \textbf{feasible} plan. Execution of the plan should be possible with the time
% and team available.  If the full plan cannot be completed during the time
% available, it can either be modified to ``fake it'', or a better solution is
% to add a section describing what work has been completed and what work is
% still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but
% before the design stage.  This means that the sections related to unit testing
% cannot initially be completed.  The sections will be filled in after the
% design stage is complete.  the final version of the VnV plan should have all
% sections filled in.}

\newpage

\tableofcontents

\listoftables
% \wss{Remove this section if it isn't needed}

\listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

The definitions of symbols, abbreviations, and acronyms used in this document
are consistent with those provided in SRS\citep{SRS}.

\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation Plan (VnV) for \progname,
structured into four main sections.  General Information includes the summary,
objectives, challenge level and extras relevant to the VnV process.  Plan
details the verification strategy for the SRS, Design, VnV, and Implementation
phases, along with the use of automated testing and verification tools, as well
as software validation plan. System Tests verify both functional and
nonfunctional requirements at the system level, ensuring the overall system
meets its intended behavior.  Unit Test Description covers validating individual
modules against both functional and nonfunctional requirements.  This plan
ensures a comprehensive and systematic approach to verifying and validating the
system before deployment.

\section{General Information}

This section will provide the project summary and objectives for this document.

\subsection{Summary}

This project focuses on reimplementing the Pan-Tompkins algorithm for R-wave
detection in ECG signal processing.  This implementation will automatically
derive all filter parameters based on requirements without relying on external
libraries.

\subsection{Objectives}

The objectives of this VnV plan include:

\begin{itemize}
  \item Build confidence in the software correctness.
  \item Test robustness against noisy ECG signals.
\end{itemize}

\noindent The objectives of this VnV plan do not include:

\begin{itemize}
  \item User experience testing for a graphical interface or integration into
  medical workflow systems.
  \item Clinical trials.
\end{itemize}

\subsection{Challenge Level and Extras}

This is a problem with a general-level challenge. Doxygen will be widely used to
generate the API documentation.

\subsection{Relevant Documentation}

In the Verification and Validation Plan, we reference the Software Requirements
Specification (SRS) \citep{SRS} as the primary document. The SRS outlines the
system’s requirements, forming the foundation for all verification activities.
It ensures that the VnV process is aligned with the specified functional and
nonfunctional requirements.

\section{Plan}

This section provides plans to verify SRS, Design, VnV, and Implementation. In
addition, some automated tools for testing are listed.

% \wss{Introduce this section.  You can provide a roadmap of the sections to
%   come.}

% \subsection{Verification and Validation Team}

% \wss{Your teammates.  Maybe your supervisor.  You should do more than list
%   names.  You should say what each person's role is for the project's
%   verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}

The SRS document \citep{SRS} will be reviewed by a domain expert.  The content
of the review is based on the following checklist:

\begin{todolist}
  \item Completeness: Included GS, A, GD, DD, TM, IM, etc.
  \item Formatting: The document has a clear, logical structure, and properly
  organized.
  \item Verifiability: Each requirement is testable with a clear pass/fail
  standard.
  \item Traceability: Each requirement is linked to an IM
\end{todolist}

\subsection{Design Verification Plan}

The Design Verification Plan will be reviewed by a domain expert.  The content
of the review is based on the following checklist:

\begin{todolist}
  \item Low module coupling: modules can be reused anywhere and do not affect
  others.
  \item Good Hierarchy: make sure the hierarchy is clear and easy to maintain.
\end{todolist}

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation Plan (This document itself) will be reviewed by
a domain expert.  The content of the review is based on the following checklist:

\begin{todolist}
  \item Completeness : VnV plan includes all necessary sections.
  \item Formatting: The document has a clear, logical structure, and properly
  organized.
  \item Appropriateness: Confirm that the methods and tools proposed in the VnV
  plan are appropriate for the project.
  \item Coverage: Make sure the VnV plan covers all the requirements.
\end{todolist}

\subsection{Implementation Verification Plan}

\begin{todolist}
  \item Static code check: Tools (cpplint) will be used to automatically detect
  common issues such as memory leaks, uninitialized variables, and other code
  quality concerns.
  \item Test coverage: Tools (gcov) will be used to check test coverage and
  ensure that most of the code is covered by tests, increasing confidence in the
  correctness of the code.
  \item Unit test: Comprehensive unit tests will be executed in testing
  framework (gTest) to ensure that every units perform correctly.
  \item Code review: The developer will conduct his own code reviews, and the
  domain expert will also participate in code reviews if possible.
\end{todolist}

% \wss{You should at least point to the tests listed in this document and the
%   unit testing plan.}

% \wss{In this section you would also give any details of any plans for static
%   verification of the implementation.  Potential techniques include code
%   walkthroughs, code inspection, static analyzers, etc.}

% \wss{The final class presentation in CAS 741 could be used as a code
% walkthrough.  There is also a possibility of using the final presentation (in
% CAS741) for a partial usability survey.}

\subsection{Automated Testing and Verification Tools}

\begin{itemize}
  \item Static code checker: cpplint - A tool that enforces Google's C++ style
  guide by checking for common coding style violations.
  \item Unit test: gTest - A C++ testing framework that provides a rich set of
  assertions and test case management.
  \item Test coverage: gcov - A test coverage analysis tool for C++ that works
  with GCC.
  \item Local code quality control: git pre-commit - A framework that runs
  checks before committing code, helping to enforce coding standards and catch
  issues early.
  \item Continuous integration: GitHub Actions - An automation tool that enables
  CI/CD workflows, allowing code to be built, tested, and deployed
  automatically.
\end{itemize}

\subsection{Software Validation Plan}

The software will be validated using the
\href{https://www.physionet.org/content/mitdb/1.0.0/}{MIT-BIH} dataset, where
the system's performance will be assessed by calculating the Root Mean Square
Error (RMSE) between the predicted and actual results. This will provide a
quantitative measure of the system's accuracy.

\section{System Tests}

% \wss{There should be text between all headings, even if it is just a roadmap
% of the contents of the subsections.}

This section will provide the system tests designed to verify requirements of
the system, including functional and non-functional requirements.

\subsection{Tests for Functional Requirements}

% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below cover the
%   requirements.  References to the SRS would be good here.}

\refstepcounter{testcasenum} \label{TC_IIR_FIR}
\subsubsection{TC\thetestcasenum : Basic IIR and FIR Filters}
		
%\paragraph{Title for Test}

\begin{enumerate}

\item{FIR Filter with Random Coefficients\\}

Control: Automatic
					
Initial State: No prior state is required; the test starts with randomly
generated filter coefficients.
					
Input:
\begin{itemize}
  \item FIR filter coefficients generated using scipy.signal.firwin(10, 0.3).
  \item A sinusoidal wave with added Gaussian noise, generated using np.sin(2 *
  np.pi * 0.05 * np.arange(1000)) + np.random.normal(0, 1, 1000).
\end{itemize}

Output:
\begin{itemize}
  \item The filtered signal produced by the implemented FIR filter should
  closely match the filtered signal from scipy.signal.lfilter.
  \item The RMSE between the two outputs should be below $1e-5$.
\end{itemize}

Test Case Derivation: The test ensures that a basic FIR filter implementation
behaves consistently with pseudo-oracle scipy.signal.
					
How test will be performed: 
\begin{enumerate}
  \item Generate FIR filter coefficients.
  \item Generate an input signal as a sinusoidal wave with Gaussian noise.
  \item Apply the FIR filter implementation.
  \item Apply scipy.signal.lfilter with the same coefficients.
  \item Compute the RMSE between outputs.
  \item Assert that the RMSE is below $1e-5$.
\end{enumerate}
					
\item{IIR Filter with Random Coefficients\\}

Control: Automatic
					
Initial State: No prior state is required; the test starts with randomly
generated filter coefficients.
					
Input:
\begin{itemize}
  \item IIR filter coefficients generated using scipy.signal.butter(3, 0.3,
  btype='low', output='ba').
  \item A sinusoidal wave with added Gaussian noise, generated using np.sin(2 *
  np.pi * 0.05 * np.arange(1000)) + np.random.normal(0, 1, 1000).
\end{itemize}

Output:
\begin{itemize}
  \item The filtered signal produced by the implemented IIR filter should
  closely match the filtered signal from scipy.signal.lfilter.
  \item The RMSE between the two outputs should be below $1e-5$.
\end{itemize}

The test ensures that a basic IIR filter implementation behaves consistently
with scipy.signal.

How test will be performed: 
\begin{enumerate}
  \item Generate IIR filter coefficients.
  \item Generate an input signal as a sinusoidal wave with Gaussian noise.
  \item Apply the IIR filter implementation.
  \item Apply scipy.signal.lfilter with the same coefficients.
  \item Compute the RMSE between outputs.
  \item Assert that the RMSE is below $1e-5$.
\end{enumerate}

\end{enumerate}

\refstepcounter{testcasenum} \label{TC_BUT_CHEB}
\subsubsection{TC\thetestcasenum : Butterworth and Chebyshev Filters}

\begin{enumerate}

  \item{Butterworth Low-Pass Filter\\}
  
  Control: Automatic
            
  Initial State: No prior state is required; the test starts with selected
  filter parameters.
            
  Input:
  \begin{itemize}
    \item Cutoff frequency: $f_c=0.3$
    \item Filter order $N=4$
    \item Filter coefficients generated using scipy.signal.butter(4, 0.3,
    btype='low', output='ba').
  \end{itemize}
  
  Output:
  \begin{itemize}
    \item The computed filter coefficients should match those generated by
    scipy.signal.butter.
    \item The RMSE between the two outputs should be below $1e-5$.
  \end{itemize}
  
  Test Case Derivation: The test ensures that a Butterworth filter
  implementation behaves consistently with pseudo-oracle scipy.signal.
            
  How test will be performed: 
  \begin{enumerate}
    \item Generate Butterworth filter coefficients by scipy.signal.butter.
    \item Generate Butterworth filter coefficients by implementation.
    \item Compute the RMSE between the implemented and scipy.signal
    coefficients.
    \item Assert that the RMSE is below 1e-5.
  \end{enumerate}
            
  \item{Chebyshev Type I Low-Pass Filter\\}
  
  Control: Automatic
            
  Initial State: No prior state is required; the test starts with selected
  filter parameters.
            
  Input:
  \begin{itemize}
    \item Cutoff frequency: $f_c=0.3$
    \item Filter order $N=4$
    \item Passband ripple: $0.5 dB$
    \item Filter coefficients generated using scipy.signal.cheby1(4, 0.5, 0.3,
    btype='low', output='ba').
  \end{itemize}
  
  Output:
  \begin{itemize}
    \item The filtered signal produced by the implemented Chebyshev filter
    should closely match the filtered signal from scipy.signal.lfilter.
    \item The RMSE between the two outputs should be below $1e-5$.
  \end{itemize}
  
  The test ensures that a Chebyshev implementation behaves consistently with
  scipy.signal.
  
  How test will be performed: 
  \begin{enumerate}
    \item Generate Chebyshev filter coefficients by scipy.signal.cheby1.
    \item Generate Chebyshev filter coefficients by implementation.
    \item Compute the RMSE between the implemented and scipy.signal
    coefficients.
    \item Assert that the RMSE is below 1e-5.
  \end{enumerate}
  
\end{enumerate}

\refstepcounter{testcasenum} \label{TC_SQUARING}
\subsubsection{TC\thetestcasenum : Squaring function}

\begin{enumerate}

  \item{Sequence input test\\}
  
  Control: Automatic
            
  Initial State: No prior state is required.
            
  Input:
  \begin{itemize}
    \item $[1.2, -2.1, -1.0, 2.0, 3.0]$
  \end{itemize}
  
  Output:
  \begin{itemize}
    \item $[1.44, 4.41, 1.0, 4.0, 9.0]$
  \end{itemize}
  
  Test Case Derivation: The purpose of this test case is to verify that the
  squaring function correctly squares each value in the input list, regardless
  of whether the numbers are positive or negative. The function should return
  the squared value of each element from the input list.

  The input given here is just an example.  In practice, the input will be a
  large amount of data randomly generated by the program for testing.
            
  How test will be performed: 
  \begin{enumerate}
    \item The input list will be provided to the squaring function.
    \item The function will process each element in the list and square it.
    \item The output will be compared against the expected result.
    \item There is a global floating point comparison threshold $\epsilon$ to
    determine whether floating point numbers are equal.
  \end{enumerate}

\end{enumerate}

\refstepcounter{testcasenum} \label{TC_THRESHOLDING}
\subsubsection{TC\thetestcasenum : Thresholding function}

\begin{enumerate}

  \item{Sequence input test\\}
  
  Control: Automatic
            
  Initial State: No prior state is required.
            
  Input:
  \begin{itemize}
    \item input sequence: $[1.44, 4.2, 1.0, 4.0, 9.0]$
    \item dynamic threshold: $[3.7, 3.8, 3.9, 3.6, 3.2]$
  \end{itemize}
  
  Output:
  \begin{itemize}
    \item $[0, 4.2, 0, 4.0, 9.0]$
  \end{itemize}
  
  Test Case Derivation: This test case aims to verify that the thresholding
  function correctly applies dynamic thresholds to each element in the input
  sequence.  For each element in the sequence, if the value is greater than the
  corresponding threshold, it should remain unchanged; otherwise, it should be
  replaced with $0$.

  The input given here is just an example.  In practice, the input will be a
  large amount of data randomly generated by the program for testing.
            
  How test will be performed: 
  \begin{enumerate}
    \item The input sequence and dynamic thresholds will be provided to the
    thresholding function.
    \item The function will process each element in the list.
    \item The output will be compared against the expected result.
    \item There is a global floating point comparison threshold $\epsilon$ to
    determine whether floating point numbers are equal.
  \end{enumerate}

\end{enumerate}

\refstepcounter{testcasenum} \label{TC_RMSE}
\subsubsection{TC\thetestcasenum : RMSE calculating function}

\begin{enumerate}

  \item{Sequence input test\\}
  
  Control: Automatic
            
  Initial State: No prior state is required.
            
  Input:
  \begin{itemize}
    \item input sequence 1: $[100, 200, 300, 400, 500]$
    \item input sequence 2: $[99, 203, 300, 398, 501]$
  \end{itemize}
  
  Output:
  \begin{itemize}
    \item $RMSE = 1.732$
  \end{itemize}
  
  Test Case Derivation: This test case verifies that the RMSE function correctly
  calculates the root mean squared error between two input sequences.  The RMSE
  is calculated as the square root of the mean of the squared differences
  between corresponding elements of the two sequences.

  The input given here is just an example.  In practice, the input will be a
  large amount of data randomly generated by the program for testing.
            
  How test will be performed: 
  \begin{enumerate}
    \item Two input sequences will be provided to the thresholding function.
    \item The function will process two input sequences and return the RMSE.
    \item The output will be compared against the expected result.
    \item There is a global floating point comparison threshold $\epsilon$ to
    determine whether floating point numbers are equal.
  \end{enumerate}

\end{enumerate}

\refstepcounter{testcasenum} \label{TC_ALG}
\subsubsection{TC\thetestcasenum : R-wave position calculating}

\begin{enumerate}

  \item{MIT-BIH Database test\\}
  
  Control: Automatic
            
  Initial State: No prior state is required.
            
  Input:
  \begin{itemize}
    \item sampling frequency $f_s=360$ Hz
    \item MIT-BIH data from
    \url{https://www.physionet.org/content/mitdb/1.0.0/}, as shown in figure
    \ref{Fig_wave}. This is a curve consisting of discrete sample points and
    contains the annotated points from cardiologists.  The blue vertical lines
    in the figure \ref{Fig_wave} are annotated points.
  \end{itemize}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=1.1\textwidth]{wave}
      \caption{input wave}
      \label{Fig_wave} 
    \end{center}
  \end{figure}
  
  Output:
  \begin{itemize}
    \item The index of annotated points from cardiologists as shown in figure
    \ref{Fig_wave}.
    \item The RMSE between calculated and annotated R-wave index.
  \end{itemize}
  
  Test Case Derivation: The test will evaluate the accuracy of the R-wave
  detection function by comparing the detected R-wave index to the annotated
  index, with the RMSE between the two sets of index expected to be below $3.0$.
            
  How test will be performed: 
  \begin{enumerate}
    \item The input $f_s$ and wave data will be provided to the calculating
    function.
    \item The annotated data will be provided to the calculating function.
    \item The function will process the original wave data and calculate the
    R-wave index.
    \item The function will return the RMSE.
    \item The correctness of this function is evaluated by the output RMSE,
    assert that the RMSE is below $3.0$.
  \end{enumerate}

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\refstepcounter{testcasenum} \label{TC_USABILITY}
\subsubsection{TC\thetestcasenum : Usability}

\begin{enumerate}

  \item{Evaluate software usability through user interaction and a survey \\}

  Type: Non-functional, Dynamic, Manual

  Initial State: The software is fully functional and ready for user testing.

  Input/Condition:
  \begin{enumerate}
    \item Users interact with the software under typical usage conditions.
    \item Conduct a user survey (section \ref{Survey}) to collect feedback on
    usability.
  \end{enumerate}

  Output/Result: Survey results and feedback from users.

  How test will be performed:
  \begin{enumerate}
    \item Distribute the program to target users for use.
    \item Conduct a user survey to gather structured feedback.
    \item Analyze survey responses and summarize findings.
  \end{enumerate}

  \item{Verify Doxygen generates a structured and readable API documentation \\}

  Type: Non-Functional, Dynamic, Manual

  Initial State: The software documentation is written in a format supported by
  Doxygen.

  Input/Condition: Run Doxygen on the project’s documentation source files.

  Output/Result: A structured and readable API documentation is generated in the
  expected format (e.g., HTML, PDF).

  How test will be performed:
  \begin{enumerate}
    \item Execute Doxygen with the configured settings.
    \item Inspect the generated documentation for completeness and readability.
    \item Verify that all sections (e.g., class descriptions, function
    documentation) are properly formatted.
    \item Conduct a peer review to assess usability.
  \end{enumerate}
\end{enumerate}

\refstepcounter{testcasenum} \label{TC_MAINTAINABILITY}
\subsubsection{TC\thetestcasenum : Maintainability}

\begin{enumerate}

  \item{Ensure high test coverage \\}

  Type: Non-Functional, Dynamic, Automatic

  Initial State: The software project contains multiple modules.

  Input/Condition: Run a test coverage analysis tool (gcov).

  Output/Result: A report showing the percentage of code covered by tests.

  How test will be performed:
  \begin{enumerate}
    \item Execute the test suite with coverage tracking enabled.
    \item Generate a test coverage report.
    \item Verify that all modules have test coverage above a predefined
    threshold.
    \item If any module lacks coverage, update tests accordingly.
  \end{enumerate}
\end{enumerate}

\refstepcounter{testcasenum} \label{TC_PORTABILITY}
\subsubsection{TC\thetestcasenum : Portability}

\begin{enumerate}

  \item{Confirm the software builds and runs correctly on multi-platform \\}

  Type: Non-Functional, Dynamic, Manual

  Initial State: The source code is available and ready for compilation.

  Input/Condition: Attempt to build and run the software on both Linux and
  Windows environments.

  Output/Result: The software compiles successfully and functions correctly on
  both platforms.

  How test will be performed:
  \begin{enumerate}
    \item Set up test environments for Linux and Windows.
    \item Compile the software on both platforms.
    \item Run key functionalities and verify expected behavior.
    \item Identify and document any platform-specific issues.
  \end{enumerate}
\end{enumerate}

\refstepcounter{testcasenum} \label{TC_REUSABILITY}
\subsubsection{TC\thetestcasenum : Reusability}

\begin{enumerate}

  \item{Check if the code is modular and reusable in different contexts \\}

  Type: Non-Functional, Static, Manual

  Initial State: The software project contains multiple modules.

  Input/Condition: Review code structure and check module usage.

  Output/Result: A checklist ensures that modules are used in different
  contexts.

  How test will be performed:
  \begin{enumerate}
    \item Inspect code structure for clear separation of concerns.
    \item Identify modules that are used in different parts of the software.
    \item Complete a module reuse checklist, and suggest improvements for better
    modularity.
  \end{enumerate}
\end{enumerate}

\newpage

\subsection{Traceability Between Test Cases and Requirements}

% \wss{Provide a table that shows which test cases are supporting which
%   requirements.}

\begin{table}[htbp!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    & FR1& FR2& FR3& FR4& FR5& FR6& FR7& FR8& NFR1& NFR2& NFR3& NFR4 \\
  \hline
  \tcref{TC_IIR_FIR}            & X& & & & & & & & & & & \\ \hline
  \tcref{TC_BUT_CHEB}           & & X& X& & & & & & & & & \\ \hline
  \tcref{TC_SQUARING}           & & & & X& & & & & & & & \\ \hline
  \tcref{TC_THRESHOLDING}       & & & & & X& & & & & & & \\ \hline
  \tcref{TC_RMSE}               & & & & & & X& & & & & & \\ \hline
  \tcref{TC_ALG}                & & & & & & & X& X& & & & \\ \hline
  \tcref{TC_USABILITY}          & & & & & & & & & X& & & \\ \hline
  \tcref{TC_MAINTAINABILITY}    & & & & & & & & & & X& & \\ \hline
  \tcref{TC_PORTABILITY}        & & & & & & & & & & & X& \\ \hline
  \tcref{TC_REUSABILITY}        & & & & & & & & & & & & X\\
  \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Requirements and System Test Cases}
  \label{Table:tc_trace}
\end{table}

\section{Unit Test Description}

This section outlines the unit testing strategy and plans for verifying the
functional components of the R-peak detection system. The tests are designed to
cover both normal and edge-case behavior for each module.

\subsection{Unit Testing Scope}

Modules in the scope:
\begin{itemize}
  \item General Digital Filter Module
  \item Specified IIR Filter Module
  \item Pan Tompkins Algorithm Module
  \item Annotated Data RMSE Module
  \item Rwave Detect Module
  \item Math Module
  \item Input Output Processing Module
  \item Error Handler Module
  \item Logger Module
\end{itemize}

Modules outside the scope:
\begin{itemize}
  \item Third-party libraries used for filtering or plotting (e.g., SciPy,
  Matplotlib).
\end{itemize}

\subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

\subsubsection{General Digital Filter Module (GDFTest)}

The General Digital Filter Module handles filter coefficient regularization and
signal filtering. The tests will consist of these two parts.

\begin{enumerate}

\item{UT-1: RegularizeTest\\}

Type: Functional, Automatic
					
Initial State: Filter uninitialized
					
Input: A set of filter parameters without regularization
					
Output: The regularized $b$ and $a$ coefficient sequences, where $a[0]$ must be
$1$.

Test Case Derivation: Each element of the $b$ and $a$ sequences is divided by
$a[0]$.

How test will be performed: Using the GoogleTest framework.

\item{UT-2: LfilterTestSuite\\}

Type: Functional, Automatic
					
Initial State: Filter initialized
					
Input: The filter parameters derived from SciPy, a signal contaminated with
Gaussian noise, and the corresponding filtered signal obtained using the
SciPy-derived parameters.
					
Output: The filtered signal computed by the General Digital Filter Module should
closely match the pseudo oracle output produced by SciPy.

Test Case Derivation: The test cases are substituted into the filter difference
equation for computation.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Specified IIR Filter Module (SIIRTest)}

The Specified IIR Filter Module includes automatic derivation of parameters for
various filters, so tests have been designed to verify the parameter derivation.

\begin{enumerate}

\item{UT-3: ButterTest\\}

Type: Functional, Automatic
					
Initial State: Filter uninitialized
					
Input: Cutoff frequency and filter order
					
Output: The calculated $b$ and $a$ coefficients for the Butterworth filter

Test Case Derivation: The correct derivation of the $b$ and $a$ coefficients
based on the specified cutoff frequency and filter order.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Pan Tompkins Algorithm Module (AlgTest)}

The Pan Tompkins Algorithm Module is responsible for detecting R-waves from
preprocessed ECG signals. Tests have been designed to verify the accuracy of
R-wave detection under various conditions.

\begin{enumerate}

\item{UT-4: RWaveDetectionTest\\}

Type: Functional, Automatic
					
Initial State: ECG signal preprocessed
					
Input: Preprocessed ECG signal
					
Output: The detected R-wave locations

Test Case Derivation: The test cases will verify the correctness of R-wave
detection by comparing the output (detected R-waves) to known ground truth
locations in the input signals.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Annotated Data RMSE Module (RMSETest)}

The Annotated Data RMSE Module calculates the Root Mean Square Error (RMSE)
between program-generated index sequences and expert-annotated sequences. Tests
have been designed to verify the correctness of the RMSE calculation.

\begin{enumerate}

\item{UT-5: calRMSESuite\\}

Type: Functional, Automatic
					
Initial State: Input sequences (program-generated and expert-annotated)
available
					
Input: Program-generated index sequence and expert-annotated index sequence
					
Output: Calculated RMSE value

Test Case Derivation: The RMSE will be calculated by comparing the
program-generated index sequence with the expert-annotated sequence, using the
standard RMSE formula.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Rwave Detect Module (RwaveDetectTest)}

The Rwave Detect Module is responsible for detecting R-wave locations from raw,
unfiltered ECG signals. The test ensures that the module correctly processes the
signal through the full filtering chain and accurately detects the R-wave
indices.

\begin{enumerate}

\item{UT-6: RWaveDetectionFilterTest\\}

Type: Functional, Automatic
					
Initial State: Raw, unfiltered ECG signal
					
Input: Unfiltered ECG signal
					
Output: Detected R-wave indices after complete filtering process

Test Case Derivation: The test will check whether the R-wave detection process
accurately identifies the R-wave locations after the entire filtering chain is
applied, comparing the detected indices to annotated data.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Math Module (MathTest)}

The Math Module focuses on performing mathematical operations, specifically
testing the squaring function. The test will ensure that the module correctly
computes the square of each element in a given input sequence.

\begin{enumerate}

\item{UT-7: calSquareSuite\\}

Type: Functional, Automatic
					
Initial State: Sequence of floating numbers available for squaring
					
Input: A sequence of floating numbers
					
Output: A sequence where each element is the square of the corresponding input
element

Test Case Derivation: The test cases will verify that each element in the input
sequence is squared correctly, ensuring the expected output for each input
value.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Input Output Processing Module (IOTest)}

The Input Output Processing Module is responsible for reading input data from
files and processing it into internal data structures. The test will ensure that
the module correctly handles file input and converts the data into the
appropriate structure.

\begin{enumerate}

\item{UT-8: read\\}

Type: Functional, Automatic
					
Initial State: Input file containing floating-point data ready for processing
					
Input: A file containing a sequence of floating-point numbers
					
Output: The corresponding data structure representing the sequence of
floating-point numbers

Test Case Derivation: The test cases will verify that the module correctly reads
the data from the input file and converts it into the expected internal data
structure without errors.

How test will be performed: Using the GoogleTest framework.

\item{UT-9: write\\}

Type: Functional, Automatic

Initial State: Internal data structure containing a sequence of floating-point
numbers

Input: The internal data structure with a sequence of floating-point numbers

Output: A file containing the corresponding sequence of floating-point numbers

Test Case Derivation: The test cases will verify that the module correctly
writes the data from the internal structure into a file, ensuring the output
file contains the expected sequence.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Error Handler Module (ErrHandlerTest)}

The Error Handler Module is responsible for catching and logging exceptions.
This test will verify that the module correctly handles exceptions and generates
appropriate log messages.

\begin{enumerate}

\item{UT-10: ErrTest\\}

Type: Functional, Automatic
					
Initial State: Exception mechanism set up in the system
					
Input: An exception (e.g., a divide-by-zero error or a file-not-found error)
					
Output: A log entry containing the exception details

Test Case Derivation: The test cases will simulate exceptions being thrown, and
the test will verify that the module correctly logs the exception message with
the appropriate details.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

\subsubsection{Logger Module (LoggerTest)}

The Error Handler Module is responsible for catching and logging exceptions.
This test will verify that the module correctly handles exceptions and generates
appropriate log messages.

\begin{enumerate}

\item{UT-11: outputTest\\}

Type: Functional, Automatic
					
Initial State: Logger initialized with a specified log file

Input: Log message and severity level (e.g., "HIGH", "LOW")

Output: The log message written to a log file with the correct severity level

Test Case Derivation: The test will verify that the module correctly logs the
message with the specified severity level to the output file. It will check that
the log file contains the expected message in the correct format.

How test will be performed: Using the GoogleTest framework.
    
\end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}
		
% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will be
%   automatic}
					
% Initial State: 
					
% Input/Condition: 
					
% Output/Result: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsubsection{Module ?}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Usability Survey Questions} \label{Survey}

\begin{enumerate}
  \item On a scale of 1-5, how easy is it to use the software?
  \begin{todolist}
    \item 1 - Very difficult
    \item 2 - Somewhat difficult
    \item 3 - Neutral
    \item 4 - Somewhat easy
    \item 5 - Very easy
  \end{todolist}
  
  \item Were you able to complete your tasks efficiently?
  \begin{todolist}
    \item Yes
    \item No
    \item Somewhat
  \end{todolist}
  
  \item Which areas of the software need improvement? (Check all that apply)
  \begin{todolist}
    \item User interface design
    \item Navigation and workflow
    \item Error handling and feedback messages
    \item Other (please specify): \underline{\hspace{5cm}}
  \end{todolist}

  \item How intuitive are the software controls and menus?
  \begin{todolist}
    \item Very intuitive
    \item Somewhat intuitive
    \item Neutral
    \item Not very intuitive
    \item Not intuitive at all
  \end{todolist}

  \item Did you encounter any usability issues? If yes, please describe them.\\
  \underline{\hspace{12cm}}
  
  \item Any additional comments or suggestions?\\
  \underline{\hspace{12cm}}
\end{enumerate}

\end{document}